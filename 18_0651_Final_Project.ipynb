{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "18.0651 Final Project",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcfc/academic_projects/blob/main/18_0651_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1xRIyQjm-TH"
      },
      "source": [
        "# 18.0651 Final Project: Can a Model Distinguish Human vs Machine?\n",
        "## by George Chen and Steven Diaz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HIz5HQhXLIp"
      },
      "source": [
        "## **0: Mount Google Drive**\n",
        "\n",
        "In your Google Drive root directory, create a folder named `180651data`, and place the 3 `.txt` files in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-rYk3LpXKNM"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.isdir('/content/drive'):\n",
        "  drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm9QnqmVn7nx"
      },
      "source": [
        "## **1\tImports**\n",
        "\n",
        "Import all the ML libraries we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpV-8IoSm9NF",
        "outputId": "6c84bd39-1df7-4025-efba-a3b16496ecf4"
      },
      "source": [
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D, Bidirectional, BatchNormalization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "word_punct_tokenizer = WordPunctTokenizer()\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn import manifold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"bread has been got\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "bread has been got\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8wnKIRYpXoR"
      },
      "source": [
        "## **2 Data**\n",
        "\n",
        "First, make functions to use **below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLzQN9_4s2AA"
      },
      "source": [
        "data_directory = 'drive/MyDrive/180651data/'\n",
        "def get_dir(filename):\n",
        "  return data_directory + filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaFKqgcMAj40"
      },
      "source": [
        "Read CSVs using pandas (executes for >= 2 mins)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jFn_tUvpmdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555c4627-d940-4888-bdd0-6e21ee42e6f0"
      },
      "source": [
        "#get original data if uncleaned, else get cleaned data\n",
        "\n",
        "# read 1/5 of the large csv\n",
        "read_factor = 5\n",
        "rand_offset = random.randint(0, read_factor - 1)\n",
        "def should_skip(index):\n",
        "  return (index + rand_offset) % read_factor != 0\n",
        "\n",
        "if not os.path.isfile(get_dir('train_cleaned.csv')):\n",
        "  print('Reading 1/{0} of train csv, with a random offset of {1}'.format(read_factor, rand_offset))\n",
        "  train = pd.read_csv(get_dir('train.txt'), sep='\\t', skiprows= lambda x: should_skip(x))\n",
        "else: \n",
        "  print('Data already cleaned; reading from train_cleaned')\n",
        "  train = pd.read_csv(get_dir('train_cleaned.csv'))\n",
        "  train.response_clean = train.response_clean.astype(str)\n",
        "\n",
        "data = train # this is data we'll use throughout code"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data already cleaned; reading from train_cleaned\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU1a0zfxHklS"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_1j9iEYRyap"
      },
      "source": [
        "Handle emojies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wOHgCtjKfk9"
      },
      "source": [
        "emojies = [\":‑)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\", \":^)\", \":っ)\", \":‑D\", \"8‑D\", \"8D\", \"x‑D\", \"xD\", \"X‑D\", \"XD\", \"=‑D\", \"=D\", \"=‑3\", \"=3\", \"B^D\", \":-))\", \">:[\", \":‑(\", \":(\", \":‑c\", \":c\", \":‑<\", \":っC\", \":<\", \":‑[\", \":[\", \":{\", \";(\", \":-||\", \":@\", \">:(\", \":'‑(\", \":'(\", \":'‑)\", \":')\", \"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D‑':\", \">:O\", \":‑O\", \":O\", \":‑o\", \":o\", \"8‑0\", \"O_O\", \"o‑o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\", \":*\", \":-*\", \":^*\", \"(\", \"}{'\", \")\", \";‑)\", \";)\", \"*-)\", \"*)\", \";‑]\", \";]\", \";D\", \";^)\", \":‑,\", \">:P\", \":‑P\", \":P\", \"X‑P\", \"x‑p\", \"xp\", \"XP\", \":‑p\", \":p\", \"=p\", \":‑Þ\", \":Þ\", \":þ\", \":‑þ\", \":‑b\", \":b\", \"d:\", \">:\\\\\", \">:/\", \":‑/\", \":‑.\", \":/\", \":\\\\\", \"=/\", \"=\\\\\", \":L\", \"=L\", \":S\", \">.<\", \":|\", \":‑|\", \":$\", \":‑X\", \":X\", \":‑#\", \":#\", \"O:‑)\", \"0:‑3\", \"0:3\", \"0:‑)\", \"0:)\", \"0;^)\", \">:)\", \">;)\", \">:‑)\", \"}:‑)\", \"}:)\", \"3:‑)\", \"3:)\", \"o/\\o\", \"^5\", \">_>^\", \"^<_<\", \"|;‑)\", \"|‑O\", \":‑J\", \":‑&\", \":&\", \"#‑)\", \"%‑)\", \"%)\", \":‑###..\", \":###..\", \"<:‑|\", \"<*)))‑{\", \"><(((*>\", \"><>\", \"\\o/\", \"*\\0/*\", \"@}‑;‑'‑‑‑\", \"@>‑‑>‑‑\", \"~(_8^(I)\", \"5:‑)\", \"~:‑\\\\\", \"//0‑0\\\\\\\\\", \"*<|:‑)\", \"=:o]\", \"7:^]\", \",:‑)\", \"</3\", \"<3\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa-ccDSZRz79"
      },
      "source": [
        "Handle contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwMo60Y3Kh3Z"
      },
      "source": [
        "cList = {\n",
        "  \"ain 't\": \"am not\",\n",
        "  \"aren 't\": \"are not\",\n",
        "  \"can 't\": \"cannot\",\n",
        "  \"can 't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could 've\": \"could have\",\n",
        "  \"couldn 't\": \"could not\",\n",
        "  \"couldn 't 've\": \"could not have\",\n",
        "  \"didn 't\": \"did not\",\n",
        "  \"doesn 't\": \"does not\",\n",
        "  \"don 't\": \"do not\",\n",
        "  \"hadn 't\": \"had not\",\n",
        "  \"hadn 't 've\": \"had not have\",\n",
        "  \"hasn 't\": \"has not\",\n",
        "  \"haven 't\": \"have not\",\n",
        "  \"he 'd\": \"he would\",\n",
        "  \"he 'd 've\": \"he would have\",\n",
        "  \"he 'll\": \"he will\",\n",
        "  \"he 'll 've\": \"he will have\",\n",
        "  \"he 's\": \"he is\",\n",
        "  \"how 'd\": \"how did\",\n",
        "  \"how 'd 'y\": \"how do you\",\n",
        "  \"how 'll\": \"how will\",\n",
        "  \"how 's\": \"how is\",\n",
        "  \"I 'd\": \"I would\",\n",
        "  \"I 'd 've\": \"I would have\",\n",
        "  \"I 'll\": \"I will\",\n",
        "  \"I 'll 've\": \"I will have\",\n",
        "  \"I 'm\": \"I am\",\n",
        "  \"I 've\": \"I have\",\n",
        "  \"isn 't\": \"is not\",\n",
        "  \"it 'd\": \"it had\",\n",
        "  \"it 'd 've\": \"it would have\",\n",
        "  \"it 'll\": \"it will\",\n",
        "  \"it 'll 've\": \"it will have\",\n",
        "  \"it 's\": \"it is\",\n",
        "  \"let 's\": \"let us\",\n",
        "  \"ma 'am\": \"madam\",\n",
        "  \"mayn 't\": \"may not\",\n",
        "  \"might 've\": \"might have\",\n",
        "  \"mightn 't\": \"might not\",\n",
        "  \"mightn 't 've\": \"might not have\",\n",
        "  \"must 've\": \"must have\",\n",
        "  \"mustn 't\": \"must not\",\n",
        "  \"mustn 't 've\": \"must not have\",\n",
        "  \"needn 't\": \"need not\",\n",
        "  \"needn 't 've\": \"need not have\",\n",
        "  \"o 'clock\": \"of the clock\",\n",
        "  \"oughtn 't\": \"ought not\",\n",
        "  \"oughtn 't 've\": \"ought not have\",\n",
        "  \"shan 't\": \"shall not\",\n",
        "  \"sha 'n 't\": \"shall not\",\n",
        "  \"shan 't 've\": \"shall not have\",\n",
        "  \"she 'd\": \"she would\",\n",
        "  \"she 'd 've\": \"she would have\",\n",
        "  \"she 'll\": \"she will\",\n",
        "  \"she 'll 've\": \"she will have\",\n",
        "  \"she 's\": \"she is\",\n",
        "  \"should 've\": \"should have\",\n",
        "  \"shouldn 't\": \"should not\",\n",
        "  \"shouldn 't 've\": \"should not have\",\n",
        "  \"so 've\": \"so have\",\n",
        "  \"so 's\": \"so is\",\n",
        "  \"that 'd\": \"that would\",\n",
        "  \"that 'd 've\": \"that would have\",\n",
        "  \"that 's\": \"that is\",\n",
        "  \"there 'd\": \"there had\",\n",
        "  \"there 'd 've\": \"there would have\",\n",
        "  \"there 's\": \"there is\",\n",
        "  \"they 'd\": \"they would\",\n",
        "  \"they 'd 've\": \"they would have\",\n",
        "  \"they 'll\": \"they will\",\n",
        "  \"they 'll 've\": \"they will have\",\n",
        "  \"they 're\": \"they are\",\n",
        "  \"they 've\": \"they have\",\n",
        "  \"to 've\": \"to have\",\n",
        "  \"wasn 't\": \"was not\",\n",
        "  \"we 'd\": \"we had\",\n",
        "  \"we 'd 've\": \"we would have\",\n",
        "  \"we 'll\": \"we will\",\n",
        "  \"we 'll 've\": \"we will have\",\n",
        "  \"we 're\": \"we are\",\n",
        "  \"we 've\": \"we have\",\n",
        "  \"weren 't\": \"were not\",\n",
        "  \"what 'll\": \"what will\",\n",
        "  \"what 'll 've\": \"what will have\",\n",
        "  \"what 're\": \"what are\",\n",
        "  \"what 's\": \"what is\",\n",
        "  \"what 've\": \"what have\",\n",
        "  \"when 's\": \"when is\",\n",
        "  \"when 've\": \"when have\",\n",
        "  \"where 'd\": \"where did\",\n",
        "  \"where 's\": \"where is\",\n",
        "  \"where 've\": \"where have\",\n",
        "  \"who 'll\": \"who will\",\n",
        "  \"who 'll 've\": \"who will have\",\n",
        "  \"who 's\": \"who is\",\n",
        "  \"who 've\": \"who have\",\n",
        "  \"why 's\": \"why is\",\n",
        "  \"why 've\": \"why have\",\n",
        "  \"will 've\": \"will have\",\n",
        "  \"won 't\": \"will not\",\n",
        "  \"won 't 've\": \"will not have\",\n",
        "  \"would 've\": \"would have\",\n",
        "  \"wouldn 't\": \"would not\",\n",
        "  \"wouldn 't 've\": \"would not have\",\n",
        "  \"y 'all\": \"you all\",\n",
        "  \"y 'alls\": \"you alls\",\n",
        "  \"y 'all 'd\": \"you all would\",\n",
        "  \"y 'all 'd 've\": \"you all would have\",\n",
        "  \"y 'all 're\": \"you all are\",\n",
        "  \"y 'all 've\": \"you all have\",\n",
        "  \"you 'd\": \"you had\",\n",
        "  \"you 'd 've\": \"you would have\",\n",
        "  \"you 'll\": \"you will\",\n",
        "  \"you 'll 've\": \"you will have\",\n",
        "  \"you 're\": \"you are\",\n",
        "  \"you 've\": \"you have\"\n",
        "}\n",
        "\n",
        "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "  def replace(match):\n",
        "    return cList[match.group(0)]\n",
        "  return c_re.sub(replace, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gt1cJvyL1Aq"
      },
      "source": [
        "Add a new column to the pandas object to contain the cleaned, pre-processed responses (executes for a long time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBVjlo1vFPMl"
      },
      "source": [
        "def preprocess_text(text):\n",
        "  text = str(text)\n",
        "  # turn to lowercase + remove '@@ '\n",
        "  text = text.lower().replace('@@ ', '')\n",
        "  # expand the contractions\n",
        "  text = expandContractions(text)\n",
        "  # remove emojies\n",
        "  for emoji in emojies:\n",
        "    text = text.replace(emoji, '')\n",
        "  # remove <..> tags\n",
        "  text = re.sub(r'<[a-zA-Z0-9_]*>', '', text)\n",
        "  # remove punctuations \n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  # remove leading + trailing whitespace\n",
        "  text = text.strip()\n",
        "\n",
        "  # stemming (not doing for now)\n",
        "  # text = PorterStemmer().stem(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "# this runs only if data is uncleaned\n",
        "if 'response_clean' not in data:\n",
        "  data['response_clean'] = data['response'].apply(lambda response: preprocess_text(response))\n",
        "  data.to_csv(get_dir('train' + '_cleaned.csv'), float_format=str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qH0euv9AcF-",
        "outputId": "56741382-9493-4ccf-eabf-c3f1a1251094"
      },
      "source": [
        "print(data.head(10))\n",
        "\n",
        "# verify data is actually clean\n",
        "print(data.at[0, 'response'])\n",
        "print(data.at[0, 'response_clean'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  ...                                     response_clean\n",
            "0           0  ...  ok  will do  do not be late though  you will m...\n",
            "1           1  ...  the fans inbetween dont have the kneejerk reac...\n",
            "2           2  ...  i m not sure if i m going to be able to do any...\n",
            "3           3  ...           i  i  i did not   looks at you curiously\n",
            "4           4  ...                       thank you so much for the rt\n",
            "5           5  ...  thaaaaaat fuccking sucks  i would just jump of...\n",
            "6           6  ...  ur serius too huh   lol its str8 dependin on w...\n",
            "7           7  ...                     that is what i m talking about\n",
            "8           8  ...                    i m sure you will find out soon\n",
            "9           9  ...        are you agreeing or is that a smart comment\n",
            "\n",
            "[10 rows x 6 columns]\n",
            "<first_speaker> <at> ok , will do - don 't be late though , you 'll miss the fun !\n",
            "ok  will do  do not be late though  you will miss the fun\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX39oIoyhJE0"
      },
      "source": [
        "#### Word Cloud Data Visualization\n",
        "Show word cloud for visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsUo1cavfJyR"
      },
      "source": [
        "# all_words = ' '.join(validation['response_clean'].values)\n",
        "# print(all_words[:50])\n",
        "# from wordcloud import WordCloud\n",
        "# wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "# plt.figure(figsize=(10, 7))\n",
        "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG87rHc7hnC-"
      },
      "source": [
        "### Semantic Feature Extraction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv1_Za2Zsb1s"
      },
      "source": [
        "Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjEj3ugMhmqK"
      },
      "source": [
        "data_responses = data['response_clean'].values.tolist()\n",
        "# MAX_NB_WORDS = 50000\n",
        "MAX_NB_WORDS = None\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(data_responses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZoxN2hAi3OI"
      },
      "source": [
        "data_sequences = tokenizer.texts_to_sequences(data_responses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4HWO5Cmi3nF",
        "outputId": "f4d64b55-91b9-4151-9bc8-d1c006c05712"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)\n",
        "print('Found %s unique tokens' % vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 198337 unique tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m018Qb-woa3K",
        "outputId": "31b6909c-7ed4-4d14-a714-3922a5a18e23"
      },
      "source": [
        "data_array = pad_sequences(data_sequences, padding='post', truncating='post')\n",
        "print('Shape of data tensor: ', data_array.shape)\n",
        "print(data_array[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor:  (1438195, 98)\n",
            "[145  34  10  10   8  25 401 146   2  34 165   3 210   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjI8xOQUsN9L"
      },
      "source": [
        "Word2Vec and Embedding\n",
        "\n",
        "mapping words to decimals based on their meaning,\n",
        "\n",
        "use pretrained model (takes >= 3.5 minutes to run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enjj4tAZsFt0"
      },
      "source": [
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "# word2vec_twitter = \"https://drive.google.com/file/d/1lw5Hr6Xw0G0bMT1ZllrtMqEgCTrM7dzc/view?usp=sharing\"\n",
        "# google_news = \"https://drive.google.com/u/0/open?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
        "\n",
        "# EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "# word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfFj90gXo35y"
      },
      "source": [
        "EMBEDDING_DIM = 300\n",
        "nb_words = min(MAX_NB_WORDS, vocab_size + 1) if MAX_NB_WORDS is not None else vocab_size + 1\n",
        "\n",
        "# # the embedding matrix\n",
        "# embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "\n",
        "# for (word, idx) in word_index.items():\n",
        "#     if word in word2vec.vocab and idx < nb_words:\n",
        "#         embedding_matrix[idx] = word2vec.word_vec(word)\n",
        "\n",
        "# print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueAVwvfpZo5K"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open(get_dir('glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGpCn7mbZpdq"
      },
      "source": [
        "EMBEDDING_DIM = 50\n",
        "embedding_matrix2 = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for (word, index) in word_index.items():\n",
        "  embeddings_vector = embeddings_index.get(word)\n",
        "  if embeddings_vector is not None:\n",
        "    embedding_matrix2[index] = embeddings_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59SK3syMAtjG"
      },
      "source": [
        "#### Word Vector Data Visualization\n",
        "Write words and word vectors to output file to use for data visualization\n",
        "\n",
        "Use website https://projector.tensorflow.org/ and click 'Load'. Then, upload the 'word_vecs.tsv' file for Step 1 for the word vectors, and upload 'words.tsv' for Step 2 to label each point\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrEVQD217c13"
      },
      "source": [
        "import csv \n",
        "\n",
        "with open('word_vecs.tsv', 'wt') as out_file:\n",
        "  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "  for (word, idx) in word_index.items():\n",
        "    if word in word2vec.vocab and idx < nb_words:\n",
        "      vec = word2vec.word_vec(word).tolist()\n",
        "      tsv_writer.writerow(vec)\n",
        "\n",
        "with open('words.tsv', 'wt') as out_file:\n",
        "  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "  for (word, idx) in word_index.items():\n",
        "    if word in word2vec.vocab and idx < nb_words:\n",
        "      tsv_writer.writerow([word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGTykzMGw_w1"
      },
      "source": [
        "#### Splitting and Labelling Data\n",
        "\n",
        "60-20-20 train-val-test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EabT-J1dxB1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3f15d2-e883-4ca5-c6ab-e8f90ccf055d"
      },
      "source": [
        "data_labels = data['human-generated'].values.astype(np.float)\n",
        "print('Original size: ', data_labels.shape[0])\n",
        "\n",
        "# first, make 80-20 split on data to create train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data_array, data_labels, train_size=0.8, shuffle=True, random_state=10\n",
        ")\n",
        "\n",
        "# then, do 75-25 split on train to create train-validation (0.75 * 0.8 = 0.6, 0.25 * 0.8 = 0.2)\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "    X_train, y_train, train_size=0.75, shuffle=True, random_state=10\n",
        ")\n",
        "\n",
        "# check we have 60-20-20 train-val-test split\n",
        "print('60%: ', data_labels.shape[0]*0.6)\n",
        "print('20%: ', data_labels.shape[0]*0.2)\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('X_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)\n",
        "print('X_validation shape: ', X_validation.shape)\n",
        "print('y_validation shape: ', y_validation.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original size:  1438195\n",
            "60%:  862917.0\n",
            "20%:  287639.0\n",
            "X_train shape:  (862917, 98)\n",
            "y_train shape:  (862917,)\n",
            "X_test shape:  (287639, 98)\n",
            "y_test shape:  (287639,)\n",
            "X_validation shape:  (287639, 98)\n",
            "y_validation shape:  (287639,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FczhNUNQvr9u"
      },
      "source": [
        "## 3 Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpNPUte9c1kG"
      },
      "source": [
        "### Neural Network Architecture (LSTM + CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC0aPv5Zvx9v"
      },
      "source": [
        "model = Sequential()\n",
        "# Embedded layer\n",
        "model.add(Embedding(input_dim=len(embedding_matrix), output_dim=EMBEDDING_DIM, weights=[embedding_matrix], \n",
        "                            input_length=data_array.shape[1], trainable=False))\n",
        "\n",
        "# Convolutional Layer\n",
        "# to be tuned later\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# LSTM Layer\n",
        "model.add(LSTM(300, return_sequences=True))\n",
        "model.add(Dropout(rate=0.5))\n",
        "model.add(LSTM(300, return_sequences=True))\n",
        "model.add(Dropout(rate=0.5))\n",
        "model.add(LSTM(300))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "nadam = optimizers.Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=nadam, metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob9-gGDnZhgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6b638c-9aab-42ff-985a-a3faaf061341"
      },
      "source": [
        "model_glove = Sequential()\n",
        "model_glove.add(Embedding(input_dim=len(embedding_matrix2), output_dim=EMBEDDING_DIM, input_length=data_array.shape[1], weights=[embedding_matrix2], trainable=True))\n",
        "model_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(BatchNormalization())\n",
        "model_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(BatchNormalization())\n",
        "model_glove.add(Bidirectional(LSTM(20)))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(BatchNormalization())\n",
        "model_glove.add(Dense(64, activation='relu'))\n",
        "model_glove.add(Dense(64, activation='relu'))\n",
        "model_glove.add(Dense(1, activation='sigmoid'))\n",
        "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model = model_glove\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 98, 50)            9916900   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 98, 40)            11360     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 98, 40)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 98, 40)            160       \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 98, 40)            9760      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 98, 40)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 98, 40)            160       \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 40)                9760      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 40)                160       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                2624      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 9,955,109\n",
            "Trainable params: 9,954,869\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QWMUdfhyBq9"
      },
      "source": [
        "### Fit the model to the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHYagBTEx6nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379356bc-cf94-4bf3-b799-3fa3e69e68fb"
      },
      "source": [
        "# batch size was 16 before\n",
        "history = dict()\n",
        "try:\n",
        "  hist = model.fit(X_train, y_train, \\\n",
        "          validation_data=(X_validation, y_validation), \\\n",
        "          epochs=50, batch_size=2048, shuffle=True, \\\n",
        "          callbacks=[])\n",
        "except: pass\n",
        "finally: history = hist.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "422/422 [==============================] - 91s 216ms/step - loss: 0.5778 - accuracy: 0.6730 - val_loss: 0.6230 - val_accuracy: 0.6237\n",
            "Epoch 2/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6024 - accuracy: 0.6506 - val_loss: 0.6760 - val_accuracy: 0.5593\n",
            "Epoch 3/50\n",
            "422/422 [==============================] - 89s 211ms/step - loss: 0.6307 - accuracy: 0.6256 - val_loss: 0.6329 - val_accuracy: 0.6230\n",
            "Epoch 4/50\n",
            "422/422 [==============================] - 88s 210ms/step - loss: 0.6429 - accuracy: 0.6153 - val_loss: 0.6399 - val_accuracy: 0.6248\n",
            "Epoch 5/50\n",
            "422/422 [==============================] - 89s 211ms/step - loss: 0.6439 - accuracy: 0.6139 - val_loss: 0.6528 - val_accuracy: 0.5946\n",
            "Epoch 6/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6506 - accuracy: 0.6039 - val_loss: 0.6513 - val_accuracy: 0.6110\n",
            "Epoch 7/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6516 - accuracy: 0.6011 - val_loss: 0.6635 - val_accuracy: 0.5876\n",
            "Epoch 8/50\n",
            "422/422 [==============================] - 89s 211ms/step - loss: 0.6448 - accuracy: 0.6118 - val_loss: 0.6432 - val_accuracy: 0.6190\n",
            "Epoch 9/50\n",
            "422/422 [==============================] - 89s 211ms/step - loss: 0.6474 - accuracy: 0.6099 - val_loss: 0.6476 - val_accuracy: 0.6137\n",
            "Epoch 10/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6497 - accuracy: 0.6072 - val_loss: 0.6524 - val_accuracy: 0.6162\n",
            "Epoch 11/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6483 - accuracy: 0.6072 - val_loss: 0.6583 - val_accuracy: 0.5766\n",
            "Epoch 12/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6473 - accuracy: 0.6102 - val_loss: 0.6518 - val_accuracy: 0.5922\n",
            "Epoch 13/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6455 - accuracy: 0.6134 - val_loss: 0.6520 - val_accuracy: 0.6275\n",
            "Epoch 14/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6455 - accuracy: 0.6126 - val_loss: 0.6498 - val_accuracy: 0.6236\n",
            "Epoch 15/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6431 - accuracy: 0.6151 - val_loss: 0.6393 - val_accuracy: 0.6251\n",
            "Epoch 16/50\n",
            "422/422 [==============================] - 88s 209ms/step - loss: 0.6410 - accuracy: 0.6178 - val_loss: 0.6353 - val_accuracy: 0.6251\n",
            "Epoch 17/50\n",
            "422/422 [==============================] - 88s 209ms/step - loss: 0.6404 - accuracy: 0.6169 - val_loss: 0.6404 - val_accuracy: 0.6188\n",
            "Epoch 18/50\n",
            "422/422 [==============================] - 89s 210ms/step - loss: 0.6401 - accuracy: 0.6186 - val_loss: 0.6524 - val_accuracy: 0.6236\n",
            "Epoch 19/50\n",
            "422/422 [==============================] - 88s 210ms/step - loss: 0.6416 - accuracy: 0.6158 - val_loss: 0.6579 - val_accuracy: 0.6261\n",
            "Epoch 20/50\n",
            "422/422 [==============================] - 88s 209ms/step - loss: 0.6433 - accuracy: 0.6130 - val_loss: 0.6452 - val_accuracy: 0.6170\n",
            "Epoch 21/50\n",
            "422/422 [==============================] - 88s 210ms/step - loss: 0.6463 - accuracy: 0.6113 - val_loss: 0.6500 - val_accuracy: 0.6129\n",
            "Epoch 22/50\n",
            "422/422 [==============================] - 88s 209ms/step - loss: 0.6516 - accuracy: 0.6037 - val_loss: 0.6577 - val_accuracy: 0.5845\n",
            "Epoch 23/50\n",
            "422/422 [==============================] - 88s 209ms/step - loss: 0.6507 - accuracy: 0.6045 - val_loss: 0.6510 - val_accuracy: 0.6139\n",
            "Epoch 24/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6508 - accuracy: 0.6035 - val_loss: 0.6469 - val_accuracy: 0.5989\n",
            "Epoch 25/50\n",
            "422/422 [==============================] - 90s 212ms/step - loss: 0.6470 - accuracy: 0.6070 - val_loss: 0.6499 - val_accuracy: 0.5415\n",
            "Epoch 26/50\n",
            "422/422 [==============================] - 90s 212ms/step - loss: 0.6445 - accuracy: 0.6112 - val_loss: 0.6463 - val_accuracy: 0.5753\n",
            "Epoch 27/50\n",
            "422/422 [==============================] - 90s 212ms/step - loss: 0.6440 - accuracy: 0.6125 - val_loss: 0.6392 - val_accuracy: 0.6252\n",
            "Epoch 28/50\n",
            "422/422 [==============================] - 90s 213ms/step - loss: 0.6439 - accuracy: 0.6148 - val_loss: 0.6443 - val_accuracy: 0.6264\n",
            "Epoch 29/50\n",
            "422/422 [==============================] - 90s 212ms/step - loss: 0.6445 - accuracy: 0.6135 - val_loss: 0.6508 - val_accuracy: 0.5504\n",
            "Epoch 30/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6454 - accuracy: 0.6123 - val_loss: 0.6411 - val_accuracy: 0.6195\n",
            "Epoch 31/50\n",
            "422/422 [==============================] - 90s 214ms/step - loss: 0.6463 - accuracy: 0.6128 - val_loss: 0.6444 - val_accuracy: 0.6177\n",
            "Epoch 32/50\n",
            "422/422 [==============================] - 90s 213ms/step - loss: 0.6466 - accuracy: 0.6103 - val_loss: 0.6499 - val_accuracy: 0.6044\n",
            "Epoch 33/50\n",
            "422/422 [==============================] - 90s 213ms/step - loss: 0.6484 - accuracy: 0.6096 - val_loss: 0.6453 - val_accuracy: 0.6210\n",
            "Epoch 34/50\n",
            "422/422 [==============================] - 90s 212ms/step - loss: 0.6488 - accuracy: 0.6086 - val_loss: 0.6438 - val_accuracy: 0.6129\n",
            "Epoch 35/50\n",
            "422/422 [==============================] - 90s 213ms/step - loss: 0.6496 - accuracy: 0.6088 - val_loss: 0.6420 - val_accuracy: 0.6263\n",
            "Epoch 36/50\n",
            "422/422 [==============================] - 90s 212ms/step - loss: 0.6477 - accuracy: 0.6116 - val_loss: 0.6464 - val_accuracy: 0.6050\n",
            "Epoch 37/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6457 - accuracy: 0.6148 - val_loss: 0.6399 - val_accuracy: 0.6218\n",
            "Epoch 38/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6451 - accuracy: 0.6147 - val_loss: 0.6403 - val_accuracy: 0.6236\n",
            "Epoch 39/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6479 - accuracy: 0.6116 - val_loss: 0.6466 - val_accuracy: 0.6156\n",
            "Epoch 40/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6490 - accuracy: 0.6063 - val_loss: 0.6458 - val_accuracy: 0.6200\n",
            "Epoch 41/50\n",
            "422/422 [==============================] - 90s 213ms/step - loss: 0.6480 - accuracy: 0.6095 - val_loss: 0.6417 - val_accuracy: 0.6049\n",
            "Epoch 42/50\n",
            "422/422 [==============================] - 90s 213ms/step - loss: 0.6448 - accuracy: 0.6122 - val_loss: 0.6474 - val_accuracy: 0.6149\n",
            "Epoch 43/50\n",
            "422/422 [==============================] - 90s 212ms/step - loss: 0.6503 - accuracy: 0.6087 - val_loss: 0.6514 - val_accuracy: 0.5756\n",
            "Epoch 44/50\n",
            "422/422 [==============================] - 89s 211ms/step - loss: 0.6518 - accuracy: 0.6077 - val_loss: 0.6468 - val_accuracy: 0.6126\n",
            "Epoch 45/50\n",
            "422/422 [==============================] - 89s 211ms/step - loss: 0.6501 - accuracy: 0.6093 - val_loss: 0.6411 - val_accuracy: 0.6224\n",
            "Epoch 46/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6488 - accuracy: 0.6106 - val_loss: 0.6406 - val_accuracy: 0.6222\n",
            "Epoch 47/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6458 - accuracy: 0.6143 - val_loss: 0.6440 - val_accuracy: 0.6217\n",
            "Epoch 48/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6447 - accuracy: 0.6159 - val_loss: 0.6420 - val_accuracy: 0.6138\n",
            "Epoch 49/50\n",
            "422/422 [==============================] - 89s 212ms/step - loss: 0.6448 - accuracy: 0.6153 - val_loss: 0.6457 - val_accuracy: 0.6085\n",
            "Epoch 50/50\n",
            "422/422 [==============================] - 89s 211ms/step - loss: 0.6429 - accuracy: 0.6163 - val_loss: 0.6431 - val_accuracy: 0.6130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7w2UUh0cjUN"
      },
      "source": [
        "### Analyze the model's accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4SDf5cO1qcV"
      },
      "source": [
        "#Results: summarize the history for accuracy\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUqBeSBe3nPY"
      },
      "source": [
        "#Summarize for loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xboYLQ0bcmKD"
      },
      "source": [
        "### Test the model on unseen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmOEvthK3p6g"
      },
      "source": [
        "# Percentage accuracy of test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.round(y_pred.flatten())\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy*100))\n",
        "res = model.evaluate(X_test, y_test)\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}